{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN+LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTMumVQnv24u"
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense, Dropout, Lambda, BatchNormalization, Input, Conv1D, TimeDistributed, Flatten, Activation, LSTM, Reshape\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, History, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as KR\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV35B6uecAyH"
      },
      "source": [
        "\n",
        " ## --- COMMUNICATION PARAMETERS ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T26gtZSDaB9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483ae982-faa2-4c6d-f5b9-2e41c26c471d"
      },
      "source": [
        "\n",
        "\n",
        "blocklength=128\n",
        "k = 1 # Bits per Symbol\n",
        "n = 2 # Channel Use\n",
        "R = k/n\n",
        "k_mod = 4  # bits per symbol / channel use\n",
        "print(\"L= \",blocklength*k_mod/n)\n",
        "L = int(blocklength*k_mod/n) # K : Information bits\n",
        "train_Eb_dB = 0 # Eb/N0 used for training\n",
        "noise_sigma = np.sqrt(1 / (2  * k_mod * R * 10 ** (train_Eb_dB / 10))) # Noise Standard Deviation\n",
        "\n",
        "batch_size = 500 # Number of messages used for training, each size = k*L\n",
        "nb_train_word = 100000\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L=  256.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Q0TYsXcHRk"
      },
      "source": [
        "## --- DATA GENERATION ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZCbxlkhaFZn"
      },
      "source": [
        "# Generate training binary Data\n",
        "train_data = np.random.randint(low=0, high=2, size=(nb_train_word, L,k))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLGMOq6cdVBY",
        "outputId": "282b414a-f742-4093-afe1-134c71e7bf7f"
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 256, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilxDM0eFcOtA"
      },
      "source": [
        "## --- MODEL HYPERPARAMETERS ---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5bdHka1azLX",
        "outputId": "04837201-eab8-420c-ce8f-13f03ec3ffed"
      },
      "source": [
        "early_stopping_patience = 100\n",
        "epochs = 100\n",
        "optimizer = Adam(lr=0.001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=early_stopping_patience)\n",
        "# Learning Rate Control\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1,\n",
        "                              patience=5, min_lr=0.0001)\n",
        "# Save the best results based on Training Set\n",
        "modelcheckpoint = ModelCheckpoint(filepath='./' + 'LSTM' + str(k) + '_' + str(L) + '_' + str(n) + '_' + str(train_Eb_dB) + 'dB' + ' ' + 'AWGN' + '.h5',\n",
        "                                  monitor='loss',\n",
        "                                  verbose=1,\n",
        "                                  save_best_only=True,\n",
        "                                  save_weights_only=True,\n",
        "                                  mode='auto', period=1)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jh7SE13hSd1"
      },
      "source": [
        "## ---DEFINING FUNCTIONS ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd0BkJRua8EE"
      },
      "source": [
        "# Define Power Norm for Tx\n",
        "def normalization(x):\n",
        "    mean = KR.mean(x ** 2)\n",
        "    return x / KR.sqrt(2 * mean)  # 2 = I and Q channels\n",
        "\n",
        "# Define Channel Layers including AWGN and Flat Rayleigh fading\n",
        "#  x: input data\n",
        "#  sigma: noise std\n",
        "def channel_layer(x, sigma):\n",
        "    w = KR.random_normal(KR.shape(x), mean=0.0, stddev=sigma)\n",
        "    return x + w\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jzMgjZcosO"
      },
      "source": [
        "## ---MODEL---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv_VRTCPa_Eu"
      },
      "source": [
        "model_input = Input(batch_shape=(batch_size, L*k,1), name='input_bits')\n",
        "\n",
        "\n",
        "#Encoding\n",
        "e = Conv1D(filters=128, strides=1, kernel_size=1 ,padding='same', name='e_0',)(model_input)\n",
        "e = BatchNormalization()(e)\n",
        "e = Activation('relu')(e)\n",
        "\n",
        "e = LSTM(units=128,return_sequences='True',name='e_1')(e)\n",
        "e = BatchNormalization()(e)\n",
        "e = Activation('relu')(e)\n",
        "\n",
        "e = TimeDistributed(Dense(n))(e)\n",
        "e = Activation('linear')(e)\n",
        "e = Reshape((int(L*n/k_mod),k_mod))(e)\n",
        "\n",
        "#Modulation\n",
        "\n",
        "e = Conv1D(filters=128, strides=1, kernel_size=1, padding=\"same\",)(e)\n",
        "e = BatchNormalization(name='e_2')(e)\n",
        "e = Activation('relu', name='e_3')(e)\n",
        "\n",
        "e = LSTM(units=64,return_sequences='True',name='e_4')(e)\n",
        "e = BatchNormalization(name='e_5')(e)\n",
        "e = Activation('tanh', name='e_6')(e)\n",
        "\n",
        "\n",
        "e = TimeDistributed(Dense(2),name='e_7')(e)\n",
        "e = Activation('linear', name='e_8')(e)\n",
        "\n",
        "e = Lambda(normalization, name='power_norm')(e)\n",
        "\n",
        "# AWGN channel\n",
        "y_h = Lambda(channel_layer, arguments={'sigma': noise_sigma}, name='channel_layer')(e)\n",
        "\n",
        "# Demodulation\n",
        "d = Conv1D(filters=128, strides=1, kernel_size=1, padding=\"same\", name='d_1')(y_h)\n",
        "d = BatchNormalization(name='d_2')(d)\n",
        "d = Activation('relu', name='d_3')(d)\n",
        "\n",
        "d = LSTM(units=64,return_sequences='True',name='d_4')(d)\n",
        "d = BatchNormalization(name='d_5')(d)\n",
        "d = Activation('tanh', name='d_6')(d)\n",
        "\n",
        "d = TimeDistributed(Dense(kmod))(d)\n",
        "d = Reshape((L,n))(d)\n",
        "\n",
        "# Decoder\n",
        "\n",
        "d = Conv1D(filters=128, strides=1, kernel_size=1 ,padding='same', name='d_7',)(d)\n",
        "d = BatchNormalization(name='d_8')(d)\n",
        "d = Activation('relu', name='d_9')(d)\n",
        "\n",
        "d = LSTM(units=128,return_sequences='True',name='d_10')(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = Activation('relu')(d)\n",
        "\n",
        "d = TimeDistributed(Dense(k),name='d_11')(d)\n",
        "d = BatchNormalization(name='d_12')(d)\n",
        "d = Reshape((L*k,1))(d)\n",
        "model_output = Activation('sigmoid', name='d_13')(d)\n",
        "\n",
        "# Build System Model\n",
        "sys_model = Model(model_input, model_output)\n",
        "encoder = Model(model_input, e)\n",
        "\n",
        "# Print Model Architecture\n",
        "sys_model.summary()\n",
        "\n",
        "# Compile Model\n",
        "sys_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# print('encoder output:', '\\n', encoder.predict(vec_one_hot, batch_size=batch_size))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOipOae4cu3c"
      },
      "source": [
        "## ---TRAINING----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPZxv9P7dNqO"
      },
      "source": [
        "print('starting train the NN...')\n",
        "start = time.clock()\n",
        "\n",
        "# TRAINING\n",
        "mod_history = sys_model.fit(label_data, label_data,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            verbose=1,\n",
        "                            validation_split=0.2, callbacks=[modelcheckpoint,reduce_lr])\n",
        "\n",
        "end = time.clock()\n",
        "\n",
        "print('The NN has trained ' + str(end - start) + ' s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ax7wK3DyJ2B"
      },
      "source": [
        "# Plot the Training Loss and Validation Loss\n",
        "hist_dict = mod_history.history\n",
        "\n",
        "val_loss = hist_dict['val_loss']\n",
        "loss = hist_dict['loss']\n",
        "acc = hist_dict['accuracy']\n",
        "# val_acc = hist_dict['val_acc']\n",
        "#print('loss:',loss)\n",
        "#print('val_loss:',val_loss)\n",
        "\n",
        "epoch = np.arange(1, epochs + 1)\n",
        "\n",
        "plt.semilogy(epoch,val_loss,label='val_loss')\n",
        "plt.semilogy(epoch, loss, label='loss')\n",
        "\n",
        "plt.legend(loc=0)\n",
        "plt.grid('true')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Binary cross-entropy loss')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhdBp-geXEJ"
      },
      "source": [
        "## --- TESTING PARAMETERS ----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZFfJMdCwJmd"
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 500 # Number of messages used for test, each size = k*L\n",
        "num_of_sym = 100000\n",
        "\n",
        "\n",
        "# Initialize information Data 0/1\n",
        "in_sym = np.random.randint(low=0, high=2, size=(num_of_sym, L,k))\n",
        "#label_data = copy.copy(in_sym)\n",
        "in_sym = np.reshape(in_sym,newshape=(num_of_sym,L*k,1))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBtXWGxWeau4"
      },
      "source": [
        "## --- TESTING DATA GENERATION ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK2LFiMdhdaB"
      },
      "source": [
        "## --- Simulation ---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLz3M6TQvYTr"
      },
      "source": [
        "ber[i] = K.mean(K.cast(K.not_equal(test_data, K.round(pred_final_signal)),dtype='float32')).numpy()\n",
        "bler = np.max((pred_final_signal.round()!=test_data),axis=1).mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F23gkwYxy1N"
      },
      "source": [
        "### BER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_IoLYRLhgyd"
      },
      "source": [
        "\n",
        "\n",
        "print('start simulation ...' + str(k) + '_' + str(L)+'_'+str(n))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        " --- DEFINE THE Neural Network(NN) ---\n",
        "'''\n",
        "\n",
        "# Eb_N0 in dB\n",
        "for Eb_N0_dB in range(0,21):\n",
        "\n",
        "    # Noise Sigma at this Eb\n",
        "    noise_sigma = np.sqrt(1 / (2 * R * kmod*10 ** (Eb_N0_dB / 10)))\n",
        "\n",
        "    # Define Encoder Layers (Transmitter)\n",
        "    model_input = Input(batch_shape=(batch_size, L*k,1), name='input_bits')\n",
        "\n",
        "\n",
        "    #Encoding\n",
        "    e = Conv1D(filters=128, strides=1, kernel_size=1 ,padding='same', name='e_0',)(model_input)\n",
        "    e = BatchNormalization()(e)\n",
        "    e = Activation('relu')(e)\n",
        "\n",
        "    e = LSTM(units=128,return_sequences='True',name='e_1')(e)\n",
        "    e = BatchNormalization()(e)\n",
        "    e = Activation('relu')(e)\n",
        "\n",
        "    e = TimeDistributed(Dense(n))(e)\n",
        "    e = Activation('linear')(e)\n",
        "    e = Reshape((int(L*n/k_mod),k_mod))(e)\n",
        "\n",
        "    #Modulation\n",
        "\n",
        "    e = Conv1D(filters=128, strides=1, kernel_size=1, padding=\"same\",)(e)\n",
        "    e = BatchNormalization(name='e_2')(e)\n",
        "    e = Activation('relu', name='e_3')(e)\n",
        "\n",
        "    e = LSTM(units=64,return_sequences='True',name='e_4')(e)\n",
        "    e = BatchNormalization(name='e_5')(e)\n",
        "    e = Activation('tanh', name='e_6')(e)\n",
        "\n",
        "\n",
        "    e = TimeDistributed(Dense(2),name='e_7')(e)\n",
        "    e = Activation('linear', name='e_8')(e)\n",
        "\n",
        "    e = Lambda(normalization, name='power_norm')(e)\n",
        "\n",
        "    # AWGN channel\n",
        "    y_h = Lambda(channel_layer, arguments={'sigma': noise_sigma}, name='channel_layer')(e)\n",
        "\n",
        "    # Demodulation\n",
        "    d = Conv1D(filters=128, strides=1, kernel_size=1, padding=\"same\", name='d_1')(y_h)\n",
        "    d = BatchNormalization(name='d_2')(d)\n",
        "    d = Activation('relu', name='d_3')(d)\n",
        "\n",
        "    d = LSTM(units=64,return_sequences='True',name='d_4')(d)\n",
        "    d = BatchNormalization(name='d_5')(d)\n",
        "    d = Activation('tanh', name='d_6')(d)\n",
        "\n",
        "    d = TimeDistributed(Dense(kmod))(d)\n",
        "    d = Reshape((L,n))(d)\n",
        "\n",
        "    # Decoder\n",
        "\n",
        "    d = Conv1D(filters=128, strides=1, kernel_size=1 ,padding='same', name='d_7',)(d)\n",
        "    d = BatchNormalization(name='d_8')(d)\n",
        "    d = Activation('relu', name='d_9')(d)\n",
        "\n",
        "    d = LSTM(units=128,return_sequences='True',name='d_10')(d)\n",
        "    d = BatchNormalization()(d)\n",
        "    d = Activation('relu')(d)\n",
        "\n",
        "    d = TimeDistributed(Dense(k),name='d_11')(d)\n",
        "    d = BatchNormalization(name='d_12')(d)\n",
        "    d = Reshape((L*k,1))(d)\n",
        "    model_output = Activation('sigmoid', name='d_13')(d)\n",
        "        \n",
        "    \n",
        "    # Build the model\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    encoder = Model(model_input,e)\n",
        "    # Load Weights from the trained NN\n",
        "    model.load_weights('./' + 'PAPER2' + str(k) + '_' + str(L) + '_' + str(n) + '_' + str(train_Eb_dB) + 'dB' + ' ' + 'AWGN' + '.h5')\n",
        "\n",
        "    '''\n",
        "    RUN THE NN\n",
        "    '''\n",
        "\n",
        "    # RUN Through the Model and get output\n",
        "    decoder_output = model.predict(label_data, batch_size=batch_size)\n",
        "    #encoder_output = encoder.predict(vec_one_hot, batch_size=batch_size)\n",
        "\n",
        "    '''\n",
        "     --- CALULATE BLER ---\n",
        "    '''\n",
        "\n",
        "    # Decode One-Hot vector\n",
        "\n",
        "    tmp = (decoder_output.round()!=label_data).reshape(label_data.shape[0],label_data.shape[1])\n",
        "    error_rate = np.mean(tmp)\n",
        "    print('Eb/N0 = ', Eb_N0_dB,'BER = ', error_rate)\n",
        "\n",
        "\n",
        "    # Store The Results\n",
        "    Vec_Eb_N0.append(Eb_N0_dB)\n",
        "    Bit_error_rate.append(error_rate)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY29rDY_x3xJ"
      },
      "source": [
        "## BLER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl39S3mfx65K"
      },
      "source": [
        "\n",
        "print('start simulation ...' + str(k) + '_' + str(L)+'_'+str(n))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        " --- DEFINE THE Neural Network(NN) ---\n",
        "'''\n",
        "\n",
        "# Eb_N0 in dB\n",
        "for Eb_N0_dB in range(0,21):\n",
        "\n",
        "    # Noise Sigma at this Eb\n",
        "    noise_sigma = np.sqrt(1 / (2 * R * 10 ** (Eb_N0_dB / 10)))\n",
        "\n",
        "    model_input = Input(batch_shape=(None, L*k,1), name='input_bits')\n",
        "\n",
        "    e = Conv1D(filters=128, strides=k, kernel_size=k, name='e_1',)(model_input)\n",
        "    e = BatchNormalization(name='e_2')(e)\n",
        "    e = Activation('relu', name='e_3')(e)\n",
        "\n",
        "    e = LSTM(units=32,return_sequences='True',name='e_4')(e)\n",
        "    e = BatchNormalization(name='e_5')(e)\n",
        "    e = Activation('tanh', name='e_6')(e)\n",
        "\n",
        "    e = TimeDistributed(Dense(2*n),name='e_7')(e)\n",
        "    e = Activation('linear', name='e_8')(e)\n",
        "    e = Reshape((e.shape[1]*n,2),name='e_9')(e)\n",
        "\n",
        "    e = Lambda(normalization, name='power_norm')(e)\n",
        "\n",
        "    # AWGN channel\n",
        "    y_h = Lambda(channel_layer, arguments={'sigma': noise_sigma}, name='channel_layer')(e)\n",
        "\n",
        "    # Define Decoder Layers (Receiver)\n",
        "    d = LSTM(units=128,return_sequences='True',name='d_1')(y_h)\n",
        "    d = BatchNormalization(name='d_2')(d)\n",
        "    d = Activation('tanh', name='d_3')(d)\n",
        "\n",
        "    d = LSTM(units=64,return_sequences='True',name='d_4')(d)\n",
        "    d = BatchNormalization(name='d_5')(d)\n",
        "    d = Activation('tanh', name='d_6')(d)\n",
        "\n",
        "    d = Conv1D(filters=64, strides=1, kernel_size=1, name='d_7')(d)\n",
        "    d = BatchNormalization(name='d_8')(d)\n",
        "    d = Activation('relu', name='d_9')(d)\n",
        "\n",
        "    d = TimeDistributed(Dense(n),name='d_10')(d)\n",
        "    d = BatchNormalization(name='d_11')(d)\n",
        "    d = Reshape((L*k,1),name='d_12')(d)\n",
        "    model_output = Activation('sigmoid', name='d_13')(d)\n",
        "    \n",
        "    \n",
        "    # Build the model\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    encoder = Model(model_input,e)\n",
        "    # Load Weights from the trained NN\n",
        "    model.load_weights('./' + 'PAPER2' + str(k) + '_' + str(L) + '_' + str(n) + '_' + str(train_Eb_dB) + 'dB' + ' ' + 'AWGN' + '.h5',\n",
        "                       by_name=False)\n",
        "\n",
        "    '''\n",
        "    RUN THE NN\n",
        "    '''\n",
        "\n",
        "    # RUN Through the Model and get output\n",
        "    decoder_output = model.predict(label_data, batch_size=batch_size)\n",
        "    #encoder_output = encoder.predict(vec_one_hot, batch_size=batch_size)\n",
        "\n",
        "    '''\n",
        "     --- CALULATE BLER ---\n",
        "    '''\n",
        "\n",
        "    # Decode One-Hot vector\n",
        "\n",
        "    resh= (decoder_output.round()!=label_data).reshape(label_data.shape[0],label_data.shape[1])\n",
        "    tmp = np.max(resh,axis=1)\n",
        "    error_rate = np.mean(tmp)\n",
        "    print('Eb/N0 = ', Eb_N0_dB,'BLER = ', error_rate)\n",
        "\n",
        "\n",
        "    # Store The Results\n",
        "    Vec_Eb_N0.append(Eb_N0_dB)\n",
        "    Block_error_rate.append(error_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyxmYMuxivAG"
      },
      "source": [
        "## --- PLOTTING ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktGjzwf1y-d6"
      },
      "source": [
        "### BER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwFnajo1it6s"
      },
      "source": [
        "\n",
        "# Print BER\n",
        "# print(Bit_error_rate)\n",
        "\n",
        "#print(Vec_Eb_N0, '\\n', Bit_error_rate)\n",
        "\n",
        "#with open('BLER_model_LBC_'+str(k)+'_'+str(n)+'_'+str(L)+'_AWGN'+'.txt', 'w') as f:\n",
        "#    print(Vec_Eb_N0, '\\n', Bit_error_rate, file=f)\n",
        "#f.closed\n",
        "\n",
        "# Plot BER Figure\n",
        "plt.semilogy(Vec_Eb_N0, Bit_error_rate, color='red')\n",
        "label = ['k= ' +str(k) + ',' + 'L= '+ str(L)]\n",
        "plt.legend(label, loc=0)\n",
        "plt.xlabel('Eb/N0')\n",
        "plt.ylabel('BER')\n",
        "plt.title('k= ' + str(k) + ', ' + 'n= ' +str(n)+', ' + 'L= ' +str(L))\n",
        "plt.grid('true')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}